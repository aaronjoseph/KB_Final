{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_Pretrained Models - BERT.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1sRIQsfakOtgjc6OITUrdL16tUFe_jFkp",
      "authorship_tag": "ABX9TyPOFd/R/4UM8zKsOJl6C+YB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fc75db752554415592067ad13bb356c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8d30f30dcb874bc8a92aca0e6f7f34ca",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c11b4d912ae0432ca9e912cf34809840",
              "IPY_MODEL_99111df635d844e88c1be6885ede75c1"
            ]
          }
        },
        "8d30f30dcb874bc8a92aca0e6f7f34ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c11b4d912ae0432ca9e912cf34809840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5ed228d964a24491b7f310086894581f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3b79771249314f5b82de5c02b56fc38c"
          }
        },
        "99111df635d844e88c1be6885ede75c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_05203844dd0e4181bd2bf009420fcdd1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:46&lt;00:00, 4.96kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec5cce97b89349b4a0ff0280979ab96d"
          }
        },
        "5ed228d964a24491b7f310086894581f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3b79771249314f5b82de5c02b56fc38c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "05203844dd0e4181bd2bf009420fcdd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec5cce97b89349b4a0ff0280979ab96d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronjoseph/KB_Final/blob/master/2_Pretrained_Models_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gxf8ay5zbub"
      },
      "source": [
        "# Main \r\n",
        "This codebase will demonstrate the following\r\n",
        "\r\n",
        "- Cleaning of the Sentiment 140 data ; tokenizing\r\n",
        "- Using pretrained model - RoBERTa to train on the data\r\n",
        "- Log the metrics over Neptune.ai\r\n",
        "- Save the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOH4QcNw8F1x"
      },
      "source": [
        "# Loading the libraries\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVOFSAdZ76Sa",
        "outputId": "e6129d1b-dff7-446e-81be-f769827eb7f5"
      },
      "source": [
        "# Tracking/Logging the data\r\n",
        "! pip install neptune-client==0.4.132\r\n",
        "! pip install transformers\r\n",
        "import neptune\r\n",
        "neptune.init(\r\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiOGI1M2QwMmItZGQyMi00MTBmLThhZTktZDY0OThjYmZlMGMyIn0=\",\r\n",
        "    project_qualified_name=\"aaronjosephmathew/sandbox\"\r\n",
        ")\r\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\r\n",
        "!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting neptune-client==0.4.132\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/15/42e29c15a2f17c76038a6d707bc6d46f6e66a9db30cc4c2d54f0712b5ae4/neptune-client-0.4.132.tar.gz (110kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 12.1MB/s \n",
            "\u001b[?25hCollecting bravado\n",
            "  Downloading https://files.pythonhosted.org/packages/6d/3d/f8772d9295c03e08a9ab4afc1ccd195efe6cb4d1af3135b7f74eb8beb0d6/bravado-11.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (7.1.2)\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 12.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (1.1.5)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (7.0.0)\n",
            "Collecting PyJWT\n",
            "  Downloading https://files.pythonhosted.org/packages/b4/9b/8850f99027ed029af6828199cc87179eaccbbf1f9e6e373e7f0177d32dad/PyJWT-2.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (1.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (1.15.0)\n",
            "Collecting websocket-client>=0.35.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 27.1MB/s \n",
            "\u001b[?25hCollecting GitPython>=2.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/cb/ec98155c501b68dcb11314c7992cd3df6dce193fd763084338a117967d53/GitPython-3.1.12-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 29.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (20.8)\n",
            "Collecting bravado-core>=5.16.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/11/18e9d28a156c33f2d5f15a5e155dc7130250acb0a569255a2b6b307b596d/bravado_core-5.17.0-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client==0.4.132) (2.8.1)\n",
            "Collecting monotonic\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client==0.4.132) (3.7.4.3)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client==0.4.132) (1.0.2)\n",
            "Collecting simplejson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/96/1e6b19045375890068d7342cbe280dd64ae73fd90b9735b5efb8d1e044a1/simplejson-3.17.2-cp36-cp36m-manylinux2010_x86_64.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 29.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client==0.4.132) (3.13)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->neptune-client==0.4.132) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas->neptune-client==0.4.132) (1.19.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client==0.4.132) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client==0.4.132) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client==0.4.132) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client==0.4.132) (2020.12.5)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->neptune-client==0.4.132) (2.4.7)\n",
            "Collecting jsonref\n",
            "  Downloading https://files.pythonhosted.org/packages/07/92/f8e4ac824b14af77e613984e480fa818397c72d4141fc466decb26752749/jsonref-0.2-py3-none-any.whl\n",
            "Requirement already satisfied: jsonschema[format]>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.4.132) (2.6.0)\n",
            "Collecting swagger-spec-validator>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/09/de/e78cefbf5838b434b63a789264b79821cb2267f1498fbed23ef8590133e4/swagger_spec_validator-2.7.3-py2.py3-none-any.whl\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Collecting rfc3987; extra == \"format\"\n",
            "  Downloading https://files.pythonhosted.org/packages/65/d4/f7407c3d15d5ac779c3dd34fbbc6ea2090f77bd7dd12f207ccf881551208/rfc3987-1.3.8-py2.py3-none-any.whl\n",
            "Collecting webcolors; extra == \"format\"\n",
            "  Downloading https://files.pythonhosted.org/packages/12/05/3350559de9714b202e443a9e6312937341bd5f79f4e4f625744295e7dd17/webcolors-1.11.1-py3-none-any.whl\n",
            "Collecting strict-rfc3339; extra == \"format\"\n",
            "  Downloading https://files.pythonhosted.org/packages/56/e4/879ef1dbd6ddea1c77c0078cd59b503368b0456bcca7d063a870ca2119d3/strict-rfc3339-0.7.tar.gz\n",
            "Building wheels for collected packages: neptune-client, future, strict-rfc3339\n",
            "  Building wheel for neptune-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neptune-client: filename=neptune_client-0.4.132-py2.py3-none-any.whl size=166223 sha256=392ad287db9dfee51bd7a5f76b926d36c24887ee7f0e552d56bd7eeb31c508b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/17/bc/2f8eef68e5adb68415d3634a6c70185860f4461b00c1c6a37b\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=99215ced1693ff18995f663495d9fcdc9cfa5adca42e2de2ca53260e2e21b136\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for strict-rfc3339 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for strict-rfc3339: filename=strict_rfc3339-0.7-cp36-none-any.whl size=18122 sha256=cc6a12bc9d4844755f79024e5183c4328112429de148386ade7419d46ccdc20c\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/af/c9/b6e9fb5f9b2470e4ed2a7241c9ab3a8cdd3bc8555ae02ca2e6\n",
            "Successfully built neptune-client future strict-rfc3339\n",
            "Installing collected packages: jsonref, simplejson, swagger-spec-validator, bravado-core, monotonic, bravado, future, PyJWT, websocket-client, smmap, gitdb, GitPython, neptune-client, rfc3987, webcolors, strict-rfc3339\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed GitPython-3.1.12 PyJWT-2.0.1 bravado-11.0.2 bravado-core-5.17.0 future-0.18.2 gitdb-4.0.5 jsonref-0.2 monotonic-1.5 neptune-client-0.4.132 rfc3987-1.3.8 simplejson-3.17.2 smmap-3.0.5 strict-rfc3339-0.7 swagger-spec-validator-2.7.3 webcolors-1.11.1 websocket-client-0.57.0\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 12.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 64.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 47.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=7745b48c4268728f5e71247bec27e7d7794725ceb62e860c0f317bde32a1c5bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  5116  100  5116    0     0  27505      0 --:--:-- --:--:-- --:--:-- 27505\n",
            "Updating... This may take around 2 minutes.\n",
            "Uninstalling torch-1.7.0+cu101:\n",
            "  Successfully uninstalled torch-1.7.0+cu101\n",
            "Uninstalling torchvision-0.8.1+cu101:\n",
            "  Successfully uninstalled torchvision-0.8.1+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "\\ [1 files][123.1 MiB/123.1 MiB]                                                \n",
            "Operation completed over 1 objects/123.1 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "| [1 files][132.1 MiB/132.1 MiB]                                                \n",
            "Operation completed over 1 objects/132.1 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][  4.9 MiB/  4.9 MiB]                                                \n",
            "Operation completed over 1 objects/4.9 MiB.                                      \n",
            "Processing ./torch-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (0.8)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.8.0a0+unknown\n",
            "Processing ./torch_xla-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+2c1ab63\n",
            "Processing ./torchvision-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (7.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.8.0a0+unknown)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly) (0.8)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.9.0a0+59d3af5\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libopenblas-dev is already the newest version (0.2.20+ds-4).\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 13 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 0s (2,646 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 146374 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bZggvZF8J-h"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "%matplotlib inline\r\n",
        "sns.set(color_codes=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og_gdMMB3t6R"
      },
      "source": [
        "# Obtaining the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS_rIrhm3tSs"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/training.1600000.processed.noemoticon.csv',header=None,error_bad_lines=False,engine='python')\r\n",
        "df.columns=['Sentiment', 'id', 'Date', 'Query', 'User', 'Tweet']\r\n",
        "df = df.drop(columns=['id', 'Date', 'Query', 'User'], axis=1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgGzmfV93YuD"
      },
      "source": [
        "df['Sentiment'] = df.Sentiment.replace(4,1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "eksCM-9t8i16",
        "outputId": "ab3c953d-50c0-499d-fbca-e2e5f1e6dc12"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "hashtags = re.compile(r\"^#\\S+|\\s#\\S+\")\r\n",
        "mentions = re.compile(r\"^@\\S+|\\s@\\S+\")\r\n",
        "urls = re.compile(r\"https?://\\S+\")\r\n",
        "\r\n",
        "def process_text(text):\r\n",
        "  text = re.sub(r'http\\S+', '', text)\r\n",
        "  text = hashtags.sub(' hashtag', text)\r\n",
        "  text = mentions.sub(' entity', text)\r\n",
        "  return text.strip().lower()\r\n",
        "  \r\n",
        "df['Tweet'] = df.Tweet.apply(process_text)\r\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>entity  - awww, that's a bummer.  you shoulda ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>entity i dived many times for the ball. manage...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>entity no, it's not behaving at all. i'm mad. ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sentiment                                              Tweet\n",
              "0          0  entity  - awww, that's a bummer.  you shoulda ...\n",
              "1          0  is upset that he can't update his facebook by ...\n",
              "2          0  entity i dived many times for the ball. manage...\n",
              "3          0     my whole body feels itchy and like its on fire\n",
              "4          0  entity no, it's not behaving at all. i'm mad. ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnjKJ1PZ8mOV"
      },
      "source": [
        "labels = df.Sentiment.values\r\n",
        "text = df.Tweet.values"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfuIIgG882si"
      },
      "source": [
        "# Model Development"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "fc75db752554415592067ad13bb356c7",
            "8d30f30dcb874bc8a92aca0e6f7f34ca",
            "c11b4d912ae0432ca9e912cf34809840",
            "99111df635d844e88c1be6885ede75c1",
            "5ed228d964a24491b7f310086894581f",
            "3b79771249314f5b82de5c02b56fc38c",
            "05203844dd0e4181bd2bf009420fcdd1",
            "ec5cce97b89349b4a0ff0280979ab96d"
          ]
        },
        "id": "1v1Kd5dT80o_",
        "outputId": "fc51f5dd-f797-4853-c099-da3bbbf135a5"
      },
      "source": [
        "from transformers import BertTokenizer,BertForSequenceClassification,AdamW\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case = True)\r\n",
        "input_ids = []\r\n",
        "attention_mask = []\r\n",
        "for i in text:\r\n",
        "    encoded_data = tokenizer.encode_plus(\r\n",
        "    i,\r\n",
        "    add_special_tokens=True,\r\n",
        "    max_length=64,\r\n",
        "    pad_to_max_length = True,\r\n",
        "    return_attention_mask= True,\r\n",
        "    return_tensors='pt')\r\n",
        "    input_ids.append(encoded_data['input_ids'])\r\n",
        "    attention_mask.append(encoded_data['attention_mask'])\r\n",
        "input_ids = torch.cat(input_ids,dim=0)\r\n",
        "attention_mask = torch.cat(attention_mask,dim=0)\r\n",
        "labels = torch.tensor(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc75db752554415592067ad13bb356c7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuJQL64-8-y2"
      },
      "source": [
        "from torch.utils.data import DataLoader,SequentialSampler,RandomSampler,TensorDataset,random_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Geujauyu9Cuv"
      },
      "source": [
        "dataset = TensorDataset(input_ids,attention_mask,labels)\r\n",
        "train_size = int(0.8*len(dataset))\r\n",
        "val_size = len(dataset) - train_size\r\n",
        "\r\n",
        "train_dataset,val_dataset = random_split(dataset,[train_size,val_size])\r\n",
        "\r\n",
        "print('Training Size - ',train_size)\r\n",
        "print('Validation Size - ',val_size)\r\n",
        "train_dl = DataLoader(train_dataset,sampler = RandomSampler(train_dataset),\r\n",
        "                     batch_size = 32)\r\n",
        "val_dl = DataLoader(val_dataset,sampler = SequentialSampler(val_dataset),\r\n",
        "                     batch_size = 32)\r\n",
        "\r\n",
        "len(train_dl),len(val_dl)\r\n",
        "\r\n",
        "model = BertForSequenceClassification.from_pretrained(\r\n",
        "'bert-base-uncased',\r\n",
        "num_labels = 2,\r\n",
        "output_attentions = False,\r\n",
        "output_hidden_states = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yz4ILbG9F_Q"
      },
      "source": [
        "import random\r\n",
        "\r\n",
        "seed_val = 17\r\n",
        "random.seed(seed_val)\r\n",
        "np.random.seed(seed_val)\r\n",
        "torch.manual_seed(seed_val)\r\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXr_z94L9PmP"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "model.to(device)\r\n",
        "\r\n",
        "print(device)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owkULzLx9SLf"
      },
      "source": [
        "optimizer = AdamW(model.parameters(),lr = 2e-5,eps=1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec42cO1W9VO8"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\r\n",
        "epochs = 1\r\n",
        "total_steps = len(train_dl)*epochs\r\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,\r\n",
        "                                           num_training_steps=total_steps)\r\n",
        "\r\n",
        "def accuracy(preds,labels):\r\n",
        "  pred_flat = np.argmax(preds,axis=1).flatten()\r\n",
        "  label_flat = labels.flatten()\r\n",
        "  return np.sum(pred_flat==label_flat)/len(label_flat)\r\n",
        "\r\n",
        "def evaluate(dataloader_test):\r\n",
        "  model.eval()\r\n",
        "  loss_val_total = 0\r\n",
        "  predictions,true_vals = [],[]\r\n",
        "  for batch in dataloader_test:\r\n",
        "      batch = tuple(b.to(device) for b in batch)\r\n",
        "      inputs = {\r\n",
        "          'input_ids':batch[0],\r\n",
        "          'attention_mask': batch[1],\r\n",
        "          'labels': batch[2]\r\n",
        "      }\r\n",
        "      with torch.no_grad():\r\n",
        "          outputs = model(**inputs)\r\n",
        "      loss = outputs[0]\r\n",
        "      logits = outputs[1]\r\n",
        "      loss_val_total += loss.item()\r\n",
        "      logits = logits.detach().cpu().numpy()\r\n",
        "      label_ids = inputs['labels'].cpu().numpy()\r\n",
        "      predictions.append(logits)\r\n",
        "      true_vals.append(label_ids)\r\n",
        "  loss_val_avg = loss_val_total / len(dataloader_test)\r\n",
        "  predictions = np.concatenate(predictions,axis=0)\r\n",
        "  true_vals = np.concatenate(true_vals,axis=0)\r\n",
        "  return loss_val_avg,predictions,true_vals\r\n",
        "\r\n",
        "if torch.cuda.is_available():  \r\n",
        "  dev = \"cuda:0\" \r\n",
        "else:  \r\n",
        "  dev = \"cpu\"  \r\n",
        "device = torch.device(dev)  \r\n",
        "a = torch.zeros(4,3)    \r\n",
        "a = a.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0eVAAVj9iAJ"
      },
      "source": [
        "import torch\r\n",
        "import sys\r\n",
        "print('__Python VERSION:', sys.version)\r\n",
        "print('__pyTorch VERSION:', torch.__version__)\r\n",
        "print('__CUDA VERSION', )\r\n",
        "from subprocess import call\r\n",
        "# call([\"nvcc\", \"--version\"]) does not work\r\n",
        "! nvcc --version\r\n",
        "print('__CUDNN VERSION:', torch.backends.cudnn.version())\r\n",
        "print('__Number CUDA Devices:', torch.cuda.device_count())\r\n",
        "print('__Devices')\r\n",
        "# call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\r\n",
        "print('Active CUDA Device: GPU', torch.cuda.current_device())\r\n",
        "print ('Available devices ', torch.cuda.device_count())\r\n",
        "print ('Current cuda device ', torch.cuda.current_device())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OjHEmue9lci"
      },
      "source": [
        "from tqdm.notebook import tqdm\r\n",
        "torch.cuda.empty_cache()\r\n",
        "for epoch in tqdm(range(1, epochs+1)):\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    loss_train_total = 0\r\n",
        "\r\n",
        "    progress_bar = tqdm(train_dl, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\r\n",
        "    for batch in progress_bar:\r\n",
        "\r\n",
        "        model.zero_grad()\r\n",
        "        \r\n",
        "        batch = tuple(b.to(device) for b in batch)\r\n",
        "        \r\n",
        "        inputs = {'input_ids':      batch[0],\r\n",
        "                  'attention_mask': batch[1],\r\n",
        "                  'labels':         batch[2],\r\n",
        "                 }       \r\n",
        "\r\n",
        "        outputs = model(**inputs)\r\n",
        "        \r\n",
        "        loss = outputs[0]\r\n",
        "        loss_train_total += loss.item()\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "        optimizer.step()\r\n",
        "        scheduler.step()\r\n",
        "        \r\n",
        "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\r\n",
        "         \r\n",
        "        \r\n",
        "    tqdm.write(f'\\nEpoch {epoch}')\r\n",
        "    \r\n",
        "    loss_train_avg = loss_train_total/len(train_dl)            \r\n",
        "    tqdm.write(f'Training loss: {loss_train_avg}')\r\n",
        "    \r\n",
        "    val_loss, predictions, true_vals = evaluate(val_dl)\r\n",
        "    val_acc = accuracy(predictions, true_vals)\r\n",
        "    tqdm.write(f'Validation loss: {val_loss}')\r\n",
        "    tqdm.write(f'Accuracy: {val_acc}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9PtBQko9ox1"
      },
      "source": [
        "output_dir = './'\r\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\r\n",
        "model_to_save.save_pretrained(output_dir)\r\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INHOpIM79raF"
      },
      "source": [
        "from transformers import BertTokenizer,BertForSequenceClassification\r\n",
        "import torch\r\n",
        "# Load the BERT tokenizer.\r\n",
        "print('Loading BERT tokenizer...')\r\n",
        "output_dir = './'\r\n",
        "tokenizer = BertTokenizer.from_pretrained(output_dir)\r\n",
        "model_loaded = BertForSequenceClassification.from_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k3QyfQn9tmg"
      },
      "source": [
        "def Sentiment(sent):\r\n",
        "    output_dir = './'\r\n",
        "    tokenizer = BertTokenizer.from_pretrained(output_dir)\r\n",
        "    model_loaded = BertForSequenceClassification.from_pretrained(output_dir)\r\n",
        "    encoded_dict = tokenizer.encode_plus(\r\n",
        "                        sent, \r\n",
        "                        add_special_tokens = True,\r\n",
        "                        max_length = 64,\r\n",
        "                        pad_to_max_length = True,\r\n",
        "                        return_attention_mask = True,\r\n",
        "                        return_tensors = 'pt',\r\n",
        "                   )\r\n",
        "        \r\n",
        "    input_id = encoded_dict['input_ids']\r\n",
        "\r\n",
        "    attention_mask = encoded_dict['attention_mask']\r\n",
        "    input_id = torch.LongTensor(input_id)\r\n",
        "    attention_mask = torch.LongTensor(attention_mask)\r\n",
        "\r\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "    model_loaded = model_loaded.to(device)\r\n",
        "    input_id = input_id.to(device)\r\n",
        "    attention_mask = attention_mask.to(device)\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        outputs = model_loaded(input_id, token_type_ids=None, attention_mask=attention_mask)\r\n",
        "\r\n",
        "    logits = outputs[0]\r\n",
        "    index = logits.argmax()\r\n",
        "    return index"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}