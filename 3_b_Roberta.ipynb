{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_b_Roberta.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1ETsVdappYzJ7Jr3xTSm-3tbRrH_uJazC",
      "authorship_tag": "ABX9TyM4+VJDDIHHlwOLM+mvDTxq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronjoseph/KB_Final/blob/master/3_b_Roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d3M3KyA805P",
        "outputId": "7a9e8a9f-ff4d-4a8c-bb33-1a1a43c6f1a5"
      },
      "source": [
        "!pip install transformers\r\n",
        "!pip install psutil\r\n",
        "!pip install neptune-client==0.4.132"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: neptune-client==0.4.132 in /usr/local/lib/python3.6/dist-packages (0.4.132)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (7.0.0)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (2.0.1)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (3.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (1.15.0)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (0.18.2)\n",
            "Requirement already satisfied: websocket-client>=0.35.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (0.57.0)\n",
            "Requirement already satisfied: bravado in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (11.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (20.8)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (2.23.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (7.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (1.1.5)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (1.3.0)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (3.1.12)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client==0.4.132) (5.17.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client==0.4.132) (1.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client==0.4.132) (3.13)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client==0.4.132) (2.8.1)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client==0.4.132) (3.17.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client==0.4.132) (3.7.4.3)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client==0.4.132) (1.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->neptune-client==0.4.132) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client==0.4.132) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client==0.4.132) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client==0.4.132) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client==0.4.132) (3.0.4)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas->neptune-client==0.4.132) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->neptune-client==0.4.132) (2018.9)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=2.0.8->neptune-client==0.4.132) (4.0.5)\n",
            "Requirement already satisfied: jsonschema[format]>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.4.132) (2.6.0)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.4.132) (2.7.3)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.6/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.4.132) (0.2)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client==0.4.132) (3.0.5)\n",
            "Requirement already satisfied: webcolors; extra == \"format\" in /usr/local/lib/python3.6/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.4.132) (1.11.1)\n",
            "Requirement already satisfied: rfc3987; extra == \"format\" in /usr/local/lib/python3.6/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.4.132) (1.3.8)\n",
            "Requirement already satisfied: strict-rfc3339; extra == \"format\" in /usr/local/lib/python3.6/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.4.132) (0.7)\n",
            "Collecting neptune-notebooks\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/07/a7c8922ea4806735a43b84a8a52b11e5e7b1ab9d9abffe94e740f19f4718/neptune-notebooks-0.0.16.tar.gz (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 12.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from neptune-notebooks) (7.1.2)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from neptune-notebooks) (0.18.2)\n",
            "Requirement already satisfied: neptune-client>=0.4.92 in /usr/local/lib/python3.6/dist-packages (from neptune-notebooks) (0.4.132)\n",
            "Requirement already satisfied: notebook>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from neptune-notebooks) (5.3.1)\n",
            "Collecting path.py\n",
            "  Downloading https://files.pythonhosted.org/packages/8f/04/130b7a538c25693c85c4dee7e25d126ebf5511b1eb7320e64906687b159e/path.py-12.5.0-py3-none-any.whl\n",
            "Requirement already satisfied: websocket-client>=0.35.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client>=0.4.92->neptune-notebooks) (0.57.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from neptune-client>=0.4.92->neptune-notebooks) (1.1.5)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.6/dist-packages (from neptune-client>=0.4.92->neptune-notebooks) (7.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from neptune-client>=0.4.92->neptune-notebooks) (20.8)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client>=0.4.92->neptune-notebooks) (2.23.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client>=0.4.92->neptune-notebooks) (1.15.0)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.6/dist-packages (from neptune-client>=0.4.92->neptune-notebooks) (3.1.12)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client>=0.4.92->neptune-notebooks) (3.1.0)\n",
            "Requirement already satisfied: bravado in /usr/local/lib/python3.6/dist-packages (from neptune-client>=0.4.92->neptune-notebooks) (11.0.2)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client>=0.4.92->neptune-notebooks) (1.3.0)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.6/dist-packages (from neptune-client>=0.4.92->neptune-notebooks) (2.0.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.0->neptune-notebooks) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.0->neptune-notebooks) (0.9.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.0->neptune-notebooks) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.0->neptune-notebooks) (5.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.0->neptune-notebooks) (2.11.2)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.0->neptune-notebooks) (5.6.1)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.0->neptune-notebooks) (5.1.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.0->neptune-notebooks) (5.1.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.0->neptune-notebooks) (4.10.1)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.0->neptune-notebooks) (4.7.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.0->neptune-notebooks) (4.3.3)\n",
            "Collecting path\n",
            "  Downloading https://files.pythonhosted.org/packages/79/07/fff7123a7fb78aaa0757a70a4ab4bc0f74796c61ddfb12a40122462eeabb/path-15.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->neptune-client>=0.4.92->neptune-notebooks) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->neptune-client>=0.4.92->neptune-notebooks) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas->neptune-client>=0.4.92->neptune-notebooks) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->neptune-client>=0.4.92->neptune-notebooks) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client>=0.4.92->neptune-notebooks) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client>=0.4.92->neptune-notebooks) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client>=0.4.92->neptune-notebooks) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client>=0.4.92->neptune-notebooks) (3.0.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=2.0.8->neptune-client>=0.4.92->neptune-notebooks) (4.0.5)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client>=0.4.92->neptune-notebooks) (5.17.0)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client>=0.4.92->neptune-notebooks) (3.17.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client>=0.4.92->neptune-notebooks) (1.0.2)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client>=0.4.92->neptune-notebooks) (1.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client>=0.4.92->neptune-notebooks) (3.7.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client>=0.4.92->neptune-notebooks) (3.13)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook>=4.3.0->neptune-notebooks) (0.7.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=5.2.0->notebook>=4.3.0->neptune-notebooks) (22.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook>=4.3.0->neptune-notebooks) (1.1.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.3.0->neptune-notebooks) (0.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.3.0->neptune-notebooks) (2.6.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.3.0->neptune-notebooks) (3.2.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.3.0->neptune-notebooks) (0.4.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.3.0->neptune-notebooks) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.3.0->neptune-notebooks) (1.4.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.3.0->neptune-notebooks) (0.8.4)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook>=4.3.0->neptune-notebooks) (2.6.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->notebook>=4.3.0->neptune-notebooks) (5.5.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook>=4.3.0->neptune-notebooks) (4.4.2)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client>=0.4.92->neptune-notebooks) (3.0.5)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client>=0.4.92->neptune-notebooks) (2.7.3)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.6/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client>=0.4.92->neptune-notebooks) (0.2)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=4.3.0->neptune-notebooks) (0.5.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook>=4.3.0->neptune-notebooks) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook>=4.3.0->neptune-notebooks) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook>=4.3.0->neptune-notebooks) (1.0.18)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook>=4.3.0->neptune-notebooks) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook>=4.3.0->neptune-notebooks) (53.0.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook>=4.3.0->neptune-notebooks) (0.2.5)\n",
            "Building wheels for collected packages: neptune-notebooks\n",
            "  Building wheel for neptune-notebooks (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neptune-notebooks: filename=neptune_notebooks-0.0.16-py2.py3-none-any.whl size=1015046 sha256=636d972eb9759feee70fb338e2a30bc7862fa73b5ce3d429dc17f32ebcfee2d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/3f/5f/7c1ebabf6e91e3eb1aacf265e69420d0a0e2ce879b6af6b8fa\n",
            "Successfully built neptune-notebooks\n",
            "Installing collected packages: path, path.py, neptune-notebooks\n",
            "Successfully installed neptune-notebooks-0.0.16 path-15.1.0 path.py-12.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLnQAl4w83AC"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import torch\r\n",
        "import seaborn as sns\r\n",
        "import transformers\r\n",
        "import neptune\r\n",
        "import json\r\n",
        "from tqdm import tqdm \r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from transformers import RobertaModel, RobertaTokenizer\r\n",
        "import logging\r\n",
        "logging.basicConfig(level=logging.ERROR)\r\n",
        "neptune.init(\r\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiOGI1M2QwMmItZGQyMi00MTBmLThhZTktZDY0OThjYmZlMGMyIn0=\",\r\n",
        "    project_qualified_name=\"aaronjosephmathew/sandbox\"\r\n",
        ")\r\n",
        "import psutil"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eJV7nCx84Gu",
        "outputId": "cb90d9f4-8ade-4cf5-d52a-581bd14ddd55"
      },
      "source": [
        "# Setting device as GPU\r\n",
        "from torch import cuda\r\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\r\n",
        "print(device)\r\n",
        "# CUDA is being used for traning"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2-YukG486CZ"
      },
      "source": [
        "# Data Loading \r\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/training.1600000.processed.noemoticon.csv',header=None,error_bad_lines=False,engine='python')\r\n",
        "df.columns=['Sentiment', 'id', 'Date', 'Query', 'User', 'Phrase']\r\n",
        "df = df.drop(columns=['id', 'Date', 'Query', 'User'], axis=1)\r\n",
        "df['Sentiment'] = df.Sentiment.replace(4,1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUi6k4X387XG"
      },
      "source": [
        "df_1 = df[['Phrase', 'Sentiment']]\r\n",
        "new_df = df_1.sample(frac=0.01,random_state=200).reset_index(drop=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LbQSMSd871E"
      },
      "source": [
        "# Key Variable Defination\r\n",
        "MAX_LEN = 256\r\n",
        "TRAIN_BATCH_SIZE = 8\r\n",
        "VALID_BATCH_SIZE = 4\r\n",
        "# EPOCHS = 1\r\n",
        "LEARNING_RATE = 1e-05\r\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voqNkxzU89KA"
      },
      "source": [
        "class SentimentData(Dataset):\r\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "        self.data = dataframe\r\n",
        "        self.text = dataframe.Phrase\r\n",
        "        self.targets = self.data.Sentiment\r\n",
        "        self.max_len = max_len\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.text)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        text = str(self.text[index])\r\n",
        "        text = \" \".join(text.split())\r\n",
        "\r\n",
        "        inputs = self.tokenizer.encode_plus(\r\n",
        "            text,\r\n",
        "            None,\r\n",
        "            add_special_tokens=True,\r\n",
        "            max_length=self.max_len,\r\n",
        "            pad_to_max_length=True,\r\n",
        "            return_token_type_ids=True\r\n",
        "        )\r\n",
        "        ids = inputs['input_ids']\r\n",
        "        mask = inputs['attention_mask']\r\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\r\n",
        "\r\n",
        "\r\n",
        "        return {\r\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\r\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\r\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\r\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\r\n",
        "        }"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfj9f1Fr8-Ru",
        "outputId": "59563472-9776-479c-b4df-0f25bec2ebf9"
      },
      "source": [
        "train_size = 0.7\r\n",
        "train_data=new_df.sample(frac=train_size,random_state=200)\r\n",
        "test_data=new_df.drop(train_data.index).reset_index(drop=True)\r\n",
        "train_data = train_data.reset_index(drop=True)\r\n",
        "\r\n",
        "print(\"FULL Dataset: {}\".format(new_df.shape))\r\n",
        "print(\"TRAIN Dataset: {}\".format(train_data.shape))\r\n",
        "print(\"TEST Dataset: {}\".format(test_data.shape))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (16000, 2)\n",
            "TRAIN Dataset: (11200, 2)\n",
            "TEST Dataset: (4800, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYON793E9CEJ"
      },
      "source": [
        "training_set = SentimentData(train_data, tokenizer, MAX_LEN)\r\n",
        "testing_set = SentimentData(test_data, tokenizer, MAX_LEN)\r\n",
        "\r\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\r\n",
        "                'shuffle': True,\r\n",
        "                'num_workers': 0\r\n",
        "                }\r\n",
        "\r\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\r\n",
        "                'shuffle': True,\r\n",
        "                'num_workers': 0\r\n",
        "                }\r\n",
        "\r\n",
        "training_loader = DataLoader(training_set, **train_params)\r\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xKTQhvm9DWg"
      },
      "source": [
        "class RobertaClass(torch.nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(RobertaClass, self).__init__()\r\n",
        "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\r\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\r\n",
        "        self.dropout = torch.nn.Dropout(0.3)\r\n",
        "        self.classifier = torch.nn.Linear(768, 2)\r\n",
        "\r\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\r\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\r\n",
        "        hidden_state = output_1[0]\r\n",
        "        pooler = hidden_state[:, 0]\r\n",
        "        pooler = self.pre_classifier(pooler)\r\n",
        "        pooler = torch.nn.ReLU()(pooler)\r\n",
        "        pooler = self.dropout(pooler)\r\n",
        "        output = self.classifier(pooler)\r\n",
        "        return output"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJCKgGIr9GDw",
        "outputId": "43b09adb-d2ea-43a8-da46-30d5a1acd8b9"
      },
      "source": [
        "model = RobertaClass()\r\n",
        "model.to(device)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaClass(\n",
              "  (l1): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcvhIIqU9Gks",
        "outputId": "e041a8bf-0c1a-4320-f471-02500c073965"
      },
      "source": [
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\r\n",
        "params = sum([np.prod(p.size()) for p in model_parameters])\r\n",
        "print(\"Number of Trainable Parameters = \",params)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Trainable Parameters =  125237762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iJfft2O9IoJ"
      },
      "source": [
        "#Loss Function & Optimizer\r\n",
        "loss_function = torch.nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\r\n",
        "\r\n",
        "def calcuate_accuracy(preds, targets):\r\n",
        "    n_correct = (preds==targets).sum().item()\r\n",
        "    return n_correct"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9OFI7fX-RHE",
        "outputId": "ee996671-7240-44da-a2ba-526f3b4dae05"
      },
      "source": [
        "neptune.create_experiment('RoBERTa')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://ui.neptune.ai/aaronjosephmathew/sandbox/e/SAN-16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Experiment(SAN-16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG2VN59p9J15"
      },
      "source": [
        "def train(epoch):\r\n",
        "    tr_loss = 0\r\n",
        "    n_correct = 0\r\n",
        "    nb_tr_steps = 0\r\n",
        "    nb_tr_examples = 0\r\n",
        "    model.train()\r\n",
        "    for _,data in tqdm(enumerate(training_loader, 0)):\r\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\r\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\r\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\r\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\r\n",
        "\r\n",
        "        outputs = model(ids, mask, token_type_ids)\r\n",
        "        loss = loss_function(outputs, targets)\r\n",
        "        tr_loss += loss.item()\r\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\r\n",
        "        n_correct += calcuate_accuracy(big_idx, targets)\r\n",
        "\r\n",
        "        nb_tr_steps += 1\r\n",
        "        nb_tr_examples+=targets.size(0)\r\n",
        "        loss_step = tr_loss/nb_tr_steps\r\n",
        "        accu_step = (n_correct*100)/nb_tr_examples\r\n",
        "\r\n",
        "        neptune.log_metric('loss_step',loss_step)\r\n",
        "        neptune.log_metric('accuracy_step',accu_step)\r\n",
        "        \r\n",
        "        if _%5000==0:\r\n",
        "            loss_step = tr_loss/nb_tr_steps\r\n",
        "            accu_step = (n_correct*100)/nb_tr_examples \r\n",
        "            print(f\"Training Loss per 5000 steps: {loss_step}\")\r\n",
        "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        # # When using GPU\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\r\n",
        "    epoch_loss = tr_loss/nb_tr_steps\r\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\r\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\r\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\r\n",
        "\r\n",
        "    return "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI8yU5539Lj6",
        "outputId": "650eabf2-8bcb-44ad-da3f-1ac0c701b296"
      },
      "source": [
        "# Keeping the model at 1 EPOCH\r\n",
        "EPOCHS = 1\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "    train(epoch)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "1it [00:00,  1.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 0.722003698348999\n",
            "Training Accuracy per 5000 steps: 37.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1400it [03:27,  6.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 0: 78.26785714285714\n",
            "Training Loss Epoch: 0.46609539333464844\n",
            "Training Accuracy Epoch: 78.26785714285714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVWDn8Ne9M65"
      },
      "source": [
        "# Validation\r\n",
        "def valid(model, testing_loader):\r\n",
        "    model.eval()\r\n",
        "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\r\n",
        "    with torch.no_grad():\r\n",
        "        for _, data in tqdm(enumerate(testing_loader, 0)):\r\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\r\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\r\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\r\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\r\n",
        "            outputs = model(ids, mask, token_type_ids).squeeze()\r\n",
        "            loss = loss_function(outputs, targets)\r\n",
        "            tr_loss += loss.item()\r\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\r\n",
        "            n_correct += calcuate_accuracy(big_idx, targets)\r\n",
        "\r\n",
        "            nb_tr_steps += 1\r\n",
        "            nb_tr_examples+=targets.size(0)\r\n",
        "            \r\n",
        "            loss_step = tr_loss/nb_tr_steps\r\n",
        "            accu_step = (n_correct*100)/nb_tr_examples\r\n",
        "            neptune.log_metric('validation_loss',loss_step)\r\n",
        "            neptune.log_metric('validation_accuracy',accu_step)\r\n",
        "      \r\n",
        "\r\n",
        "            if _%5000==0:\r\n",
        "                loss_step = tr_loss/nb_tr_steps\r\n",
        "                accu_step = (n_correct*100)/nb_tr_examples\r\n",
        "                print(f\"Validation Loss per 100 steps: {loss_step}\")\r\n",
        "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\r\n",
        "    epoch_loss = tr_loss/nb_tr_steps\r\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\r\n",
        "    print(f\"Validation Loss Epoch: {epoch_loss}\")\r\n",
        "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\r\n",
        "    \r\n",
        "    return epoch_accu\r\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5ui8Shc9O4A",
        "outputId": "55557b12-ce19-4376-8f97-8b4148be4780"
      },
      "source": [
        "acc = valid(model, testing_loader)\r\n",
        "print(\"Accuracy on test data = %0.2f%%\" % acc)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "5it [00:00,  4.50it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Loss per 100 steps: 0.23497247695922852\n",
            "Validation Accuracy per 100 steps: 100.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1200it [00:28, 42.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Loss Epoch: 0.37633210376215476\n",
            "Validation Accuracy Epoch: 83.10416666666667\n",
            "Accuracy on test data = 83.10%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qErqjtHm9QFF"
      },
      "source": [
        "torch.save(model.state_dict(),'model_dict.ckpt')\r\n",
        "neptune.log_artifact('model_dict.ckpt')"
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}